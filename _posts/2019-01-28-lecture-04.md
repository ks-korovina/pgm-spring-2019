---
layout: distill
title: "Lecture 4: Exact inference"
description: Introducing the problem of inference and finding exact solutions to it in graphical models
date: 2019-01-28

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Tingfung Lau  # author's full name; let's order by sections
    url: "#"  # optional URL to the author's homepage
  - name: Austin Dill
    url: "#"
  - name: Lingxiao Zhao
    url: "#"
  - name: Ksenia Korovina
    url: "#"

editors:
  - name: Xun Zheng  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

## Introduction

## Elimination Algorithm and Examples

Now that we understand the problem of inference, we will examine some simple cases to build intuition for a general method for exact inference.

### Elimination on Chains

Consider a simple chain on variables $A, B, C, D, E$ as seen below. 

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-04/chain1.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Chain PGM.</strong>
  </figcaption>
</figure>

Imagine we want the probability of $E=e$ regardless of the values of $A, B, C, D$. Naively, we could sum over the joint probability:

$$
P(e) = \sum_d \sum_c \sum_b \sum_a P(a, b, c, d, e)
$$

This will require an exponential number of terms. Thankfully, we can use the properties of Bayesian Networks to cut down on this computational cost. Since Bayesian Networks encode conditional independences, we can decompose the joint probability as follows:

$$
P(e) = \sum_d \sum_c \sum_b \sum_a P(a) P (b | a) P(c | b) P(d | c) P(e | d)
$$

This decomposition has allowed us to decouple conditionally independent variables and we can therefore push in and isolate summations, like the following:

$$
P(e) = \sum_d \sum_c \sum_b P(c | b) P(d | c) P(e | d) \sum_a P(a) P (b | a)
$$

Focusing on the final term, $\sum_a P(a)P(b\vert a)$, we see that this marginalizes over $a$ and leaves us with a function of only $b$. We will generally refer to this as $\phi(b)$ but semantically it is equivalent to $P(b)$. We are left with the following expression for the marginal probability of $e$. 

$$
P(e) = \sum_d \sum_c \sum_b P(c | b) P(d | c) P(e | d) P(b)
$$

Note that because the variable $a$ is no longer part of this expression we will say $a$ has been *eliminated*. We are therefore left with a new graphical model for our situation:

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-04/chain2.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Graphical Model after Elimination of A.</strong>
  </figcaption>
</figure>

Repeating this, we get the following sequence of steps:


<d-math block>
\begin{aligned}
P(e) &= \sum_d \sum_c P(d | c) P(e | d) \sum_b P(c | b) P(b) \\
&= \sum_d \sum_c P(d | c) P(e | d) P(c) \\
&= \sum_d P(e | d) \sum_c P(d | c) P(c) \\
&= \sum_d P(e | d) P(d) \\
\end{aligned}
</d-math>


As each elimination step costs $O(Val\vert X_i \vert \times \vert X_{i+1} \vert)$, the overall complexity is $O(nk^2)$, a huge improvement overall from the exponential runtime of the naive summation of the joint probability. 

### Elimination in Hidden Markov Models

Now we will consider a model frequently used in time-series analysis and Natural Language Processing known as a Hidden Markov Model. 

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-04/HMM.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Hidden Markov Model.</strong>
  </figcaption>
</figure>

Naively we could find the conditional probability of $y_i$ given the observed sequence, but using our elimination trick, we can get similar computational advantages as seen in the chain example. 


<d-math block>
\begin{aligned}
P(y_i | x_1, \dots, x_T) &= \sum_{y_{-i}} P(y_1, \dots, y_T, x_1, \dots, x_T) \\
&= \sum_{y_{-i}} P(y_1) P(x_1 | y_1) P(y_2 | y_1) \dots P(y_T | y_{T-1}) P(x_T | y_T) \\  
\end{aligned}
</d-math>


With this model, we have two intuitive choices for the order of variables to eliminate. We could start from the first time step (known as the *Forward Algorithm*) or start from the final time step (known as the *Backward Algorithm*).

Note that to each notation, we will represent a summation over all random variables $y$ except the $i$th variable as $y_{-i}$.

#### Forward Algorithm

If we choose to eliminate variables by starting at the beginning of the chain, we would first group factors as follows:


<d-math block>
\begin{aligned}
P(y_i | x_1, \dots, x_T) &= \sum_{y_{-1, -i}} P(x_2 | y_2) P(y_3 | y_2) \dots P(y_T | y_{T-1}) P(x_T | y_T) \sum_{y_1} P(y_1) P(x_1 | y_1) P(y_2 | y_1)\\  
&=  \sum_{y_{-1, -i}} P(x_2 | y_2) P(y_3 | y_2) \dots P(y_T | y_{T-1}) P(x_T | y_T) \phi(x_1, y_2) \\  
&=  \sum_{y_{-1, -i}} P(x_2 | y_2) P(y_3 | y_2) \dots P(y_T | y_{T-1}) P(x_T | y_T) P(x_1, y_2) \\  
\end{aligned}
</d-math>

We can continue in this pattern with each intermediate term $\phi(\cdot)$ representing a joint probability. 


#### Backward Algorithm

If we choose to eliminate variables by starting at the end of the chain, we would first group factors as follows:


<d-math block>
\begin{aligned}
P(y_i | x_1, \dots, x_T) &= \sum_{y_{-T, -i}} P(y_1) P(x_1 | y_1) P(y_2 | y_1) \dots P(x_{T-1} | y_{T-1}) P(y_{T-1} | y_{T-2}) \sum_{y_T} P(y_T | y_{T-1}) P(x_T | y_T) \\  
&= \sum_{y_{-T, -i}} P(y_1) P(x_1 | y_1) P(y_2 | y_1) \dots P(x_{T-1} | y_{T-1}) P(y_{T-1} | y_{T-2}) \phi(x_T, y_{T-1})
&= \sum_{y_{-T, -i}} P(y_1) P(x_1 | y_1) P(y_2 | y_1) \dots P(x_{T-1} | y_{T-1}) P(y_{T-1} | y_{T-2}) \P(x_T | y_{T-1})  
\end{aligned}
</d-math>

We can continue in this pattern with each intermediate term $\phi(\cdot)$ representing a conditional probability. 

### Takeaways from Examples

The main takeaways from our exploration are that elimination provides a systematic way to efficiently do exact inference and that while we can generally create intermediate factors, the semantics of the intermediate factors can vary. 

## Generalizing Elimination

(Graph elimination here)

---

## Message Passing Algorithms

#### Overview ---- OK

Now we have devised a general Eliminate algorithm that is able to work on every graph. However, it has several downsides. One of them, as we have discussed [TODO for the previous scribe: MENTION IT], is exponential worst case complexity. Another one is that it is designed to only answer single-node queries. In this section, we build on the same idea of exploiting local structure of a graph to manipulate factors, and formulate a class of exact inference algorithms based on passing messages over the Clique tree data structure. Doing so will give us important insight on the way inference works in general, and also provide computational benefits in the case when multiple queries have to be computed based on the same evidence. Next, we will show that the message-passing idea can be implemented more efficiently for the special case tree-like graphs. Finally, we conclude with a summary of exact inference.

This section will provide just a cursory overview of the aforementioned techniques, with the intent of presenting intuitions about how they connect to one another, and also clearing up some confusing terminology. For more in-depth explanations and proofs for each of the topics, the scribe would advise looking into the references.

[Mention that the graph is preprocessed into factor form? if DGM -> moralize]


#### Variable elimination and Clique Trees -- OK

[cit Koller, Murphy]

(1 - Koller, first pages up to sum product - connect to Eliminate (basically understanding eliminate part of the slides))

Let us start by drawing a connection between variable elimination process as we have seen in Eliminate algorithm, and a special data structure called a Clique tree (also known by the names of Junction or Join tree). Recall that performing one step of variable elimination involved creating a factor $\psi_i$ by multiplying several existing factors, and then one variable is summed out of $\psi_i$ to create a factor $\tau_i$ that is sent to other factors as a "message". A run of this algorithm defines a *Clique tree*: it is an undirected graph with nodes corresponding to factors $\psi_i$, or cliques of variables $C_i$ in its scope; an edge between $C_i$ and $C_j$ is added if a message $\tau_i$ is used to in the computation of $C_j$'s message $\tau_j$. Notice that any message $\tau_i$ is passed only once: when a factor $\phi_i$ is used to create the next factor $\psi_j$, it is never used again; hence this graph can be seen to be a tree. Figure [FIG] presents an example [TODO: maybe add Fig 10.1 on p.346 of Koller, and the corresponding VE run].

[FIGURE]

A more algorithmically principled way of constructing a clique tree given the elimination order triangulates $\mathcal{G}$ into its chordal graph, extracts max cliques in it and and finds a minimal spanning tree in the resulting clique graph. Triangulation "triangulates" the graph by iteratively adding an edge between any non-adjacent vertices into any cycle of length at least $4$. Equivalently, given elimination order, we can add edges that would be added if we were to run elimination. Next, maximal cliques $C_i$ are extracted from this graph and arranged in a graph with edges $s_{i,j} = C_i\cap C_j$. Finally, a minimum spanning tree in this graph is a clique tree. The initial graph, triangulated graph, max cliques and the corresponding clique tree are presented in the below example [FIG].

[FIGURE]

Moreover, there is a simple characterization of exactly those trees with $C_i \subseteq X$ as nodes and $S_{i,j} \subseteq C_i\cap C_j$ as edges that are clique trees defined by some variable elimination procedure (using a property called Running Intersections; TODO: family preservation?). This lets us identify clique trees with executions of variable elimination. As we will see, interpreting variable elimination in terms of clique trees has several computational advantages. For example, one tree may be used as a basis for executing several different elimination orderings. Furthermore, it makes it possible to cache intermediate messages for answering multiple marginal probability queries more efficiently.


#### General Sum-Product on a Clique Tree -- OK

[cite Koller, Murphy]

The Sum-Product algorithm provides a way to use a Clique tree to guide variable elimination. Starting with a clique tree $\Tau$ with cliques $C_i$, we perform the following steps:

1. Generate initial potentials by multiplying factors assigned to each clique $C_i$:

$$\psi_i(C_i) = \prod \psi$$

2. Choose root $C_r$ to be a clique that contains variable of interest.
3. Orient the graph upward towards the root. This defines partial ordering of operations on the tree.
4. Pass messages bottom-up (collect evidence phase): in the topological order from leaves to root, compute and store

$$\delta_{i\rightarrow j} = \sum_{C_i - S_{i,j}} \psi_i \cdot \prod_{k\in Pred(i)} \delta_{k\rightarrow i}$$

5. Distribute messages top-down (distribute evidence phase): for each clique $C_i$

$$\beta_i = \psi_r \cdot \prod_{k\in Pred(r)} \delta_{k\rightarrow r}$$

After step $4$, we can get the marginal for the root node, $P(C_r)$, by multiplying the incoming messages with $C_r$'s own clique potential:

$$P(C_r) = \psi_r \cdot \prod_{k\in Pred(r)} \delta_{k\rightarrow r}$$

(to get the likelihood of the variable of interest, it remains to sum out irrelevant variables). However, the benefit of using the clique tree shows after the top-down phase of step $5$, after which the *beliefs* $\beta_i$ we obtain are actually equal to marginals $P(C_i)$ for every clique. This way, running $N$ marginal queries is reduced from $Nc$ to just $2c$ (at the cost of storing the tree and messages between bottom-up and top-down passes).

There are several modifications of the algorithm. One replaces sum-product with max-product in the collect evidence phase, and with traceback in the distribute evidence phase, to produce a MAP estimate for $\mathcal{G}$. Another gives a way to do posterior sampling on from the model, by having the collect evidence phase proceed as usual, and on the distribute evidence phase sampling variables given the values higher up the tree.

The resulting *Clique (Junction) tree algorithm* is a general algorithm for exact inference. However, it inherits the worst-case complexity of the Eliminate algorithm, which is exponential in the size of the largest clique in elimination order. The smallest size of largest clique over all elimination orderings is called the treewidth of a graph; it captures the complexity of VE, as well as CTA. However, finding the ordering, as well as the treewidth itself, is NP-hard in general. This limits the applicability of both of these algorithms.

Next, we will present a more specialized instantiation of a message-passing algorithm that is limited to trees or tree-like structures, but is more efficient. Moreover, it can be applied to non-trees in an iterative fashion, resulting in an approximate inference algorithm known as *Loopy Belief propagation*.


#### Sum-Product algorithm on trees [cite Murphy 20.1, Jordan ch4] -- PROGRESS

(3 - Constructing junction tree is computationally expensive (NP-hard); for trees/polytrees, there is more efficient algorithm; sum-product is also known as BP)

BP on trees (20.1) - maybe just simply say that use tree instead of clique tree? Forward-backward algorithm is the same algorithm, but applied to chains.

Max-product for MAP inference can be used, as before

Loopy BP -  practically useful


#### Summary of exact inference [cite Murphy, Jordan] -- OK

Let us recap what we have learnt about exact inference. We have seen Eliminate, Clique tree, and Sum-Product algorithms. Eliminate algorithm is conceptually simple and applicable to any graphical model. However, it only lets us compute single queries and has worst case exponential time complexity in treewidth. Clique tree algorithm is also applicable to general graphs and is able to fix the first of Eliminate's issues by caching computation using messages, but has the same computational complexity as a function of graph properties. Sum-product algorithm can be thought of as implementing the same idea of passing messages around the graph and can thus be for several-query applications, but reduces the computational complexity of Clique tree algorithm at the cost of being limited to tree-like graphical models.

In general, the above trade-offs between generality and computational complexity are unavoidable: it can be shown that exact inference is NP-hard (Dagum and Luby 1993). In general, this intractability of exact inference leads to the need for approximate inference algorithms, which we will study later in the course. However, it is worth understanding exact inference. For one, as we have seen, under some assumptions about the model's structure exact inference is feasible, for example when the model is a tree. This case turns out to be quite important in applications, as many interesting models have a tree-like structure [Jordan]. Moreover, some of the approximate inference algorithms, such as Loopy BP, are inspired by exact inference algorithms.


#### Some obscure stuff I will take care of at the last moment

(4 - Minka?)

On the level of an extended abstract perhaps

