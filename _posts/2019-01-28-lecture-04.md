---
layout: distill
title: "Lecture 4: Exact inference"
description: Introducing the problem of inference and finding exact solutions to it in graphical models
date: 2019-01-28

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Tingfung Lau  # author's full name; let's order by sections
    url: "#"  # optional URL to the author's homepage
  - name: Austin Dill
    url: "#"
  - name: Lingxiao Zhao
    url: "#"
  - name: Ksenia Korovina
    url: "#"

editors:
  - name: Xun Zheng  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

## Introduction

## Elimination Algorithm and Examples

Now that we understand the problem of inference, we will examine some simple cases to build intuition for a general method for exact inference.

### Elimination on Chains

Consider a simple chain on variables $A, B, C, D, E$ as seen below. 

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-04/chain1.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Chain PGM.</strong>
  </figcaption>
</figure>

Imagine we want the probability of $E=e$ regardless of the values of $A, B, C, D$. Na\"{i}vely, we could sum over the joint probability:

$$
P(e) = \sum_d \sum_c \sum_b \sum_a P(a, b, c, d, e)
$$

This will require an exponential number of terms. Thankfully, we can use the properties of Bayesian Networks to cut down on this computational cost. Since Bayesian Networks encode conditional independences, we can decompose the joint probability as follows:

$$
P(e) = \sum_d \sum_c \sum_b \sum_a P(a) P (b | a) P(c | b) P(d | c) P(e | d)
$$

This decomposition has allowed us to decouple conditionally independent variables and we can therefore push in and isolate summations, like the following:

$$
P(e) = \sum_d \sum_c \sum_b P(c | b) P(d | c) P(e | d) \sum_a P(a) P (b | a)
$$

Focusing on the final term, $\sum_a P(a)P(b\vert a)$, we see that this marginalizes over $a$ and leaves us with a function of only $b$. We will generally refer to this as $\phi(b)$ but semantically it is equivalent to $P(b)$. We are left with the following expression for the marginal probability of $e$. 

$$
P(e) = \sum_d \sum_c \sum_b P(c | b) P(d | c) P(e | d) P(b)
$$

Note that because the variable $a$ is no longer part of this expression we will say $a$ has been \textit{eliminated}. We are therefore left with a new graphical model for our situation:

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-04/chain2.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Graphical Model after Elimination of A.</strong>
  </figcaption>
</figure>

Repeating this, we get the following sequence of steps:

<d-math block>
\begin{aligned}
P(e) &= \sum_d \sum_c P(d | c) P(e | d) \sum_b P(c | b) P(b) \\
&= \sum_d \sum_c P(d | c) P(e | d) P(c) \\
&= \sum_d P(e | d) \sum_c P(d | c) P(c) \\
&= \sum_d P(e | d) P(d) \\
\end{aligned}
</d-math>

As each elimination step costs $O(Val\vert X_i \vert \times \vert X_{i+1} \vert)$, the overall complexity is $O(nk^2)$, a huge improvement overall from the exponential runtime of the naive summation of the joint probability. 

### Elimination in Hidden Markov Models

Now we will consider a model frequently used in time-series analysis and Natural Language Processing known as a Hidden Markov Model. 

Na\"{i}vely we could find the conditional probability of $y_i$ given the observed sequence, but using our elimination trick, we can get similar computational advantages as seen in the chain example.

With this model, we have two intuitive choices for the order of variables to eliminate. We could start from the first time step (known as the \textit{Forward Algorithm}) or start from the final time step (known as the \textit{Backward Algorithm}).

## Generalizing Elimination 

## Message Passing
